{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fab6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic dependencies\n",
    "\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from numpy import savetxt\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "###########\n",
    "\n",
    "# torch dependencies\n",
    "import torch\n",
    "\n",
    "tkwargs = {\"dtype\": torch.double, # set as double to minimize zero error for cholesky decomposition error\n",
    "           \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")} # set tensors to GPU, if multiple GPUs please set cuda:x properly\n",
    "\n",
    "torch.set_printoptions(precision=3)\n",
    "\n",
    "###########\n",
    "\n",
    "# botorch dependencies\n",
    "import botorch\n",
    "\n",
    "# data related\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "\n",
    "# surrogate model specific\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "\n",
    "# qNEHVI specific\n",
    "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qNoisyExpectedHypervolumeImprovement\n",
    "\n",
    "# utilities\n",
    "from botorch.optim.optimize import optimize_acqf\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "from botorch.utils.multi_objective.hypervolume import Hypervolume\n",
    "from typing import Optional\n",
    "from torch import Tensor\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "###########\n",
    "\n",
    "# pymoo dependencies\n",
    "import pymoo\n",
    "\n",
    "from pymoo.problems import get_problem\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "\n",
    "from pymoo.algorithms.moo.unsga3 import UNSGA3\n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.optimize import minimize\n",
    "\n",
    "from pymoo.core.problem import Problem as PymooProblem\n",
    "from pymoo.core.termination import NoTermination\n",
    "\n",
    "###########\n",
    "\n",
    "# jmetalpy dependencies\n",
    "from jmetal.core.problem import FloatProblem\n",
    "from jmetal.core.solution import FloatSolution\n",
    "from jmetal.util.termination_criterion import StoppingByEvaluations, TerminationCriterion\n",
    "from jmetal.util.aggregative_function import Tschebycheff\n",
    "from jmetal.operator import PolynomialMutation, DifferentialEvolutionCrossover\n",
    "from jmetal.algorithm.multiobjective.moead import Permutation\n",
    "from jmetal.algorithm.multiobjective import MOEADIEpsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7e79c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_qnehvi(problem, ref_point, initial_x, # must haves\n",
    "                    N_BATCH, BATCH_SIZE, \n",
    "                    random_state=torch.randint(1000000, (1,)).item(), noise=0, verbose=False): # change noise here!\n",
    "    \n",
    "    print(\"Optimizing with Pure qNEHVI\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # some initializing \n",
    "    torch.manual_seed(random_state) # gives a consistent seed based on the trial number\n",
    "    hv=Hypervolume(ref_point=-ref_point) # sets the hv based on problem, flip since BoTorch takes maximisation\n",
    "    hvs = [] # create a blank array to append the scores at each batch/iteration for that run\n",
    "    \n",
    "    ##########\n",
    "    # generate initial training data for that run\n",
    "    train_x = initial_x\n",
    "    train_obj, train_con = problem.evaluate(train_x)\n",
    "\n",
    "    # add noise, by default noise=0, so train_noisy = train, noise factor determines amt of std dev to add\n",
    "    train_obj_noisy = train_obj + noise*torch.randn_like(train_obj)\n",
    "    train_con_noisy = train_con + noise*torch.randn_like(train_con)\n",
    "    \n",
    "    ##########\n",
    "    \n",
    "    # normalize inputs to [0,1] first before feeding into model\n",
    "    standard_bounds = torch.zeros(2, problem.n_var, **tkwargs)\n",
    "    standard_bounds[1] = 1\n",
    "    train_x_gp = normalize(train_x, problem.bounds)\n",
    "    \n",
    "    # form the output train_y data by concentenating ground truth train_obj with its feasibility value on the rightmost\n",
    "    # this is necessary since the surrogate GpyTorch model needs to model BOTH obj and con for predicted candidates\n",
    "    train_y = torch.cat([train_obj_noisy, train_con_noisy], dim=-1) # model takes noisy observations\n",
    "\n",
    "    # define and train surrogate models for objective and constraint\n",
    "    models = []\n",
    "    for i in range(train_y.shape[-1]):\n",
    "        models.append(SingleTaskGP(train_x_gp, train_y[..., i : i + 1], outcome_transform=Standardize(m=1)))\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "        \n",
    "    ##########    \n",
    "    \n",
    "    def create_idxr(i):\n",
    "        def idxr(Z):\n",
    "            return Z[..., i]\n",
    "\n",
    "        return idxr\n",
    "\n",
    "    def create_idxrs():\n",
    "        return [create_idxr(i=i) for i in range(problem.n_obj, problem.n_obj+problem.n_constr)]\n",
    "    \n",
    "    # original location for an extra HV check wrt to initial samples\n",
    "    \n",
    "    ########## ########## ########## start of iteration loop\n",
    "\n",
    "\n",
    "    # training loop for N_BATCH iterations\n",
    "    for iteration in range(1, N_BATCH + 1):    \n",
    "\n",
    "        t3 = time.time()\n",
    "                \n",
    "        # fit the surrogate model\n",
    "        fit_gpytorch_model(mll)    \n",
    "                \n",
    "        ##########\n",
    "            \n",
    "        # define the acqusition function for EIC if feas_weighting is false\n",
    "        acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "            model=model,\n",
    "            ref_point=-ref_point, # for computing HV, must flip for BoTorch\n",
    "            X_baseline=train_x_gp, # feed total list of train_x for this current iteration\n",
    "            sampler=SobolQMCNormalSampler(num_samples=128),  # determines how candidates are randomly proposed before selection\n",
    "            objective=IdentityMCMultiOutputObjective(outcomes=np.arange(problem.n_obj).tolist()), # optimize first n_obj col \n",
    "            constraints=create_idxrs(), # constraint on last n_constr col\n",
    "            prune_baseline=True, cache_pending=True)  # options for improving qNEHVI, keep these on\n",
    "        \n",
    "        ##########\n",
    "        \n",
    "        # propose candidates given defined qNEHVI acq func given model and latest observed training data\n",
    "        new_x, _ = optimize_acqf(\n",
    "                        acq_function=acq_func,\n",
    "                        bounds=standard_bounds, # since train_x was normalized\n",
    "                        q=BATCH_SIZE, # no of candidates to propose in parallel\n",
    "                        num_restarts=2, # no of restarts of raw_samples\n",
    "                        raw_samples=256,  # pool of samples to choose the starting points from\n",
    "                        options={\"batch_limit\": 5, \"maxiter\": 200}, # default arguments, not too sure about this yet\n",
    "                        )\n",
    "\n",
    "        # unormalize our training inputs back to original problem bounds\n",
    "        new_x =  unnormalize(new_x.detach(), bounds=problem.bounds)\n",
    "\n",
    "        # feed new proposed observations into objective func to get its new ground truth\n",
    "        new_obj, new_con = problem.evaluate(new_x)\n",
    "\n",
    "        # add noise, by default noise=0, so train_noisy = train, noise factor determines amt of std dev to add\n",
    "        new_obj_noisy = new_obj + noise*torch.randn_like(new_obj)\n",
    "        new_con_noisy = new_con + noise*torch.randn_like(new_con)\n",
    "\n",
    "        # update training points by concatenating the new values into their respective tensors\n",
    "        train_x = torch.cat([train_x, new_x])\n",
    "        train_obj = torch.cat([train_obj, new_obj])\n",
    "        train_con = torch.cat([train_con, new_con])\n",
    "        train_obj_noisy = torch.cat([train_obj_noisy, new_obj_noisy])\n",
    "        train_con_noisy = torch.cat([train_con_noisy, new_con_noisy])\n",
    "        \n",
    "        ##########\n",
    "        \n",
    "        # computing HV of current candidate list\n",
    "        is_feas = (train_con <= 0).all(dim=-1) # check whether points fit ALL (.all) constraint criteria\n",
    "        feas_train_obj = train_obj[is_feas] # take only points that fit the 1st check\n",
    "        if feas_train_obj.shape[0] > 0:\n",
    "            pareto_mask = is_non_dominated(feas_train_obj) # check for 2nd criteria: non-dominated, meaning new pareto optimal\n",
    "            pareto_y = feas_train_obj[pareto_mask] # take only points that fit the 2nd check\n",
    "            volume = hv.compute(pareto_y) # compute change in HV with new pareto optimal wrt to original ref point\n",
    "        else:\n",
    "            volume = 0.0\n",
    "        \n",
    "        hvs.append(volume)\n",
    "        \n",
    "        ##########\n",
    "\n",
    "        # update the surrogate models for next iteration\n",
    "        train_x_gp = normalize(train_x, problem.bounds) # dont forget to renormalize!\n",
    "        train_y = torch.cat([train_obj_noisy, train_con_noisy], dim=-1) # model takes noisy observations\n",
    "\n",
    "        models = []\n",
    "        for i in range(train_y.shape[-1]):\n",
    "            models.append(SingleTaskGP(train_x_gp, train_y[..., i : i + 1], outcome_transform=Standardize(m=1)))\n",
    "        model = ModelListGP(*models)\n",
    "        mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "        \n",
    "        ##########\n",
    "        \n",
    "        t4 = time.time()\n",
    "        if verbose:\n",
    "            print(\n",
    "                    f\"Batch {iteration:>2} of {N_BATCH}: Hypervolume = \"\n",
    "                    f\"{hvs[-1]:>4.2f}, \"\n",
    "                    f\"time = {t4-t3:>4.2f}s.\\n\"\n",
    "                    , end=\"\")\n",
    "            \n",
    "        del new_x, new_obj, new_con, new_obj_noisy, new_con_noisy, train_y\n",
    "        torch.cuda.empty_cache() # clear some memory here between each run/trial     \n",
    "        \n",
    "        ########## ########## ########## end of iteration loop\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"Time taken in total: {t1-t0:>4.2f}s.\")       \n",
    "    \n",
    "    # returns the HV score across iterations, total training set as an array\n",
    "    return hvs, torch.hstack([train_x, train_obj, train_con]).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96301717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hybrid_nsga(problem, ref_point, initial_x, # must haves\n",
    "                         N_BATCH, BATCH_SIZE, \n",
    "                         random_state=torch.randint(1000000, (1,)).item(), noise=0, verbose=False): # change noise here!\n",
    "    \n",
    "    print(\"Optimizing with Hybrid qNEHVI + U-NSGA-III\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    torch.manual_seed(random_state) # gives a consistent seed based on the trial number\n",
    "    hv=Hypervolume(ref_point=-ref_point) # sets the hv based on problem, flip since BoTorch takes maximisation\n",
    "    hvs = [] # create a blank array to append the scores at each batch/iteration for that run\n",
    "    \n",
    "    pymooproblem = PymooProblem(n_var=problem.n_var, n_obj=problem.n_obj, n_constr=problem.n_constr, \n",
    "                  xl=np.zeros(problem.n_var), xu=np.ones(problem.n_var))\n",
    "\n",
    "    ##########\n",
    "    # generate initial training data for that run\n",
    "    train_x = initial_x\n",
    "    train_obj, train_con = problem.evaluate(train_x)\n",
    "\n",
    "    # add noise, by default noise=0, so train_noisy = train, noise factor determines amt of std dev to add\n",
    "    train_obj_noisy = train_obj + noise*torch.randn_like(train_obj)\n",
    "    train_con_noisy = train_con + noise*torch.randn_like(train_con)\n",
    "\n",
    "    ##########\n",
    "\n",
    "    # normalize inputs to [0,1] first before feeding into model\n",
    "    standard_bounds = torch.zeros(2, problem.n_var, **tkwargs)\n",
    "    standard_bounds[1] = 1\n",
    "    train_x_gp = normalize(train_x, problem.bounds)\n",
    "\n",
    "    # form the output train_y data by concentenating ground truth train_obj with its feasibility value on the rightmost\n",
    "    # this is necessary since the surrogate GpyTorch model needs to model BOTH obj and con for predicted candidates\n",
    "    train_y = torch.cat([train_obj_noisy, train_con_noisy], dim=-1) # model takes noisy observations\n",
    "\n",
    "    # define and train surrogate models for objective and constraint\n",
    "    models = []\n",
    "    for i in range(train_y.shape[-1]):\n",
    "        models.append(SingleTaskGP(train_x_gp, train_y[..., i : i + 1], outcome_transform=Standardize(m=1)))\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    \n",
    "    ##########    \n",
    "    \n",
    "    def create_idxr(i):\n",
    "        def idxr(Z):\n",
    "            return Z[..., i]\n",
    "\n",
    "        return idxr\n",
    "\n",
    "    def create_idxrs():\n",
    "        return [create_idxr(i=i) for i in range(problem.n_obj, problem.n_obj+problem.n_constr)]\n",
    "    \n",
    "    ########## ########## ########## start of iteration loop\n",
    "\n",
    "    for iteration in range(1, N_BATCH + 1):   \n",
    "\n",
    "        t3 = time.time()\n",
    "\n",
    "        ##########\n",
    "\n",
    "        fit_gpytorch_model(mll)  \n",
    "\n",
    "        # define the acqusition function for EIC if feas_weighting is false\n",
    "        acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "            model=model,\n",
    "            ref_point=-ref_point, # for computing HV, must flip for BoTorch\n",
    "            X_baseline=train_x_gp, # feed total list of train_x for this current iteration\n",
    "            sampler=SobolQMCNormalSampler(num_samples=128),  # determines how candidates are randomly proposed before selection\n",
    "            objective=IdentityMCMultiOutputObjective(outcomes=np.arange(problem.n_obj).tolist()), # optimize first n_obj col \n",
    "            constraints=create_idxrs(), # constraint on last n_constr col\n",
    "            prune_baseline=True, cache_pending=True)  # options for improving qNEHVI, keep these on\n",
    "\n",
    "        # propose best candidates given QMC and qNEHVI\n",
    "        qnehvi_x, _ = optimize_acqf(acq_function=acq_func,\n",
    "                                    bounds=standard_bounds, # since train_x was normalized\n",
    "                                    q=BATCH_SIZE, # no of candidates to propose in parallel, 12 is the max for a GTX1065\n",
    "                                    num_restarts=1, # no of restarts if q candidates fail to show improvement\n",
    "                                    raw_samples=256,  # pool of samples to choose the starting points from\n",
    "                                    options={\"batch_limit\": 5, \"maxiter\": 200}, # default arguments, not too sure about this yet\n",
    "                                 )\n",
    "\n",
    "        ##########\n",
    "       \n",
    "        algorithm = UNSGA3(pop_size=256,\n",
    "                           ref_dirs=get_reference_directions(\"energy\", problem.n_obj, 256, seed=random_state),\n",
    "                           sampling=train_x_gp.cpu().numpy(),\n",
    "                          )\n",
    "\n",
    "        algorithm.setup(pymooproblem, termination=NoTermination())\n",
    "\n",
    "        # set the 1st population to the current evaluated population\n",
    "        pop = algorithm.ask()\n",
    "        pop.set(\"F\", train_obj_noisy.cpu().numpy())\n",
    "        pop.set(\"G\", train_con_noisy.cpu().numpy())\n",
    "        algorithm.tell(infills=pop)\n",
    "\n",
    "        # propose children based on tournament selection -> crossover/mutation\n",
    "        newpop = algorithm.ask()\n",
    "        nsga3_x = torch.tensor(newpop.get(\"X\"), **tkwargs)\n",
    "        \n",
    "        ##########\n",
    "\n",
    "        candidates = torch.cat([qnehvi_x, nsga3_x])\n",
    "\n",
    "        acq_value_list = []\n",
    "\n",
    "        for i in range(0, candidates.shape[0]):\n",
    "            with torch.no_grad():\n",
    "                acq_value = acq_func(candidates[i].unsqueeze(dim=0))\n",
    "                acq_value_list.append(acq_value.item())\n",
    "\n",
    "        pred_hv_list = []\n",
    "        model.eval();\n",
    "\n",
    "        for i in range(0, candidates.shape[0]):\n",
    "            with torch.no_grad():\n",
    "                posterior = model.posterior(candidates[i].unsqueeze(0))\n",
    "                pred_y = posterior.mean\n",
    "                pred_hv = hv.compute(pred_y[:,:problem.n_obj])\n",
    "                pred_hv_list.append(pred_hv)\n",
    "\n",
    "        sorted_x = candidates.cpu().numpy()[np.lexsort((pred_hv_list, acq_value_list))]\n",
    "\n",
    "        ##########\n",
    "        \n",
    "        new_x = torch.tensor(sorted_x[-BATCH_SIZE:], **tkwargs) # take best BATCH_SIZE samples\n",
    "        new_x =  unnormalize(new_x.detach(), bounds=problem.bounds)\n",
    "        new_obj, new_con = problem.evaluate(new_x)\n",
    "\n",
    "        # add noise, by default noise=0, so train_noisy = train, noise factor determines amt of std dev to add\n",
    "        new_obj_noisy = new_obj + noise*torch.randn_like(new_obj)\n",
    "        new_con_noisy = new_con + noise*torch.randn_like(new_con)\n",
    "\n",
    "        # update training points by concatenating the new values into their respective tensors\n",
    "        train_x = torch.cat([train_x, new_x])\n",
    "        train_obj = torch.cat([train_obj, new_obj])\n",
    "        train_con = torch.cat([train_con, new_con])\n",
    "        train_obj_noisy = torch.cat([train_obj_noisy, new_obj_noisy])\n",
    "        train_con_noisy = torch.cat([train_con_noisy, new_con_noisy])\n",
    "\n",
    "        # computing HV of current candidate list\n",
    "        is_feas = (train_con <= 0).all(dim=-1) # check whether points fit ALL (.all) constraint criteria\n",
    "        feas_train_obj = train_obj[is_feas] # take only points that fit the 1st check\n",
    "        if feas_train_obj.shape[0] > 0:\n",
    "            pareto_mask = is_non_dominated(feas_train_obj) # check for 2nd criteria: non-dominated, meaning new pareto optimal\n",
    "            pareto_y = feas_train_obj[pareto_mask] # take only points that fit the 2nd check\n",
    "            volume = hv.compute(pareto_y) # compute change in HV with new pareto optimal wrt to original ref point\n",
    "        else:\n",
    "            volume = 0.0\n",
    "\n",
    "        hvs.append(volume)\n",
    "\n",
    "        ##########\n",
    "\n",
    "        # update the surrogate models for next iteration\n",
    "        train_x_gp = normalize(train_x, problem.bounds) # dont forget to renormalize!\n",
    "        train_y = torch.cat([train_obj_noisy, train_con_noisy], dim=-1) # model takes noisy observations\n",
    "\n",
    "        models = []\n",
    "        for i in range(train_y.shape[-1]):\n",
    "            models.append(SingleTaskGP(train_x_gp, train_y[..., i : i + 1], outcome_transform=Standardize(m=1)))\n",
    "        model = ModelListGP(*models)\n",
    "        mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "        ##########\n",
    "\n",
    "        t4 = time.time()\n",
    "        if verbose:\n",
    "            print(\n",
    "                    f\"Batch {iteration:>2} of {N_BATCH}: Hypervolume = \"\n",
    "                    f\"{hvs[-1]:>4.2f}, \"\n",
    "                    f\"time = {t4-t3:>4.2f}s.\\n\"\n",
    "                    , end=\"\")\n",
    "\n",
    "        del new_x, new_obj, new_con, qnehvi_x, nsga3_x, new_obj_noisy, new_con_noisy, train_y\n",
    "        torch.cuda.empty_cache() # clear some memory here between each run/trial    \n",
    "        \n",
    "    t1 = time.time()\n",
    "    print(f\"Time taken in total: {t1-t0:>4.2f}s.\")   \n",
    "    \n",
    "    del models, model, mll\n",
    "    torch.cuda.empty_cache() # clear some memory here between each run/trial  \n",
    "    \n",
    "    # returns the HV score across iterations, total training set as an array\n",
    "    return hvs, torch.hstack([train_x, train_obj, train_con]).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5700df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hybrid_moead(problem, ref_point, initial_x, # must haves\n",
    "                         N_BATCH, BATCH_SIZE, \n",
    "                         random_state=torch.randint(1000000, (1,)).item(), noise=0, verbose=False): # change noise here!\n",
    "    \n",
    "    print(\"Optimizing with Hybrid qNEHVI + MOEAD\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    torch.manual_seed(random_state) # gives a consistent seed based on the trial number\n",
    "    hv=Hypervolume(ref_point=-ref_point) # sets the hv based on problem, flip since BoTorch takes maximisation\n",
    "    hvs = [] # create a blank array to append the scores at each batch/iteration for that run\n",
    "    \n",
    "    # define jmetal class\n",
    "    class jmetalproblem(FloatProblem):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(jmetalproblem, self).__init__()\n",
    "            self.number_of_variables = problem.n_var\n",
    "            self.number_of_objectives = problem.n_obj\n",
    "            self.number_of_constraints = problem.n_constr\n",
    "\n",
    "            obj_directions = []\n",
    "            obj_labels = []\n",
    "\n",
    "            for i in range(1,problem.n_obj+1):\n",
    "                obj_directions.append(self.MINIMIZE)\n",
    "                obj_labels.append(f'f{i}')\n",
    "\n",
    "            self.obj_directions = obj_directions\n",
    "            self.obj_labels = obj_labels\n",
    "\n",
    "            self.lower_bound = [0.0] * problem.n_var\n",
    "            self.upper_bound = [1.0] * problem.n_var\n",
    "\n",
    "        def evaluate(self, solution: FloatSolution) -> FloatSolution:\n",
    "            pass\n",
    "\n",
    "        def get_name(self):\n",
    "            return 'jmetalproblem'\n",
    "\n",
    "    ##########\n",
    "    # generate initial training data for that run\n",
    "    train_x = initial_x\n",
    "    train_obj, train_con = problem.evaluate(train_x)\n",
    "\n",
    "    # add noise, by default noise=0, so train_noisy = train, noise factor determines amt of std dev to add\n",
    "    train_obj_noisy = train_obj + noise*torch.randn_like(train_obj)\n",
    "    train_con_noisy = train_con + noise*torch.randn_like(train_con)\n",
    "\n",
    "    ##########\n",
    "\n",
    "    # normalize inputs to [0,1] first before feeding into model\n",
    "    standard_bounds = torch.zeros(2, problem.n_var, **tkwargs)\n",
    "    standard_bounds[1] = 1\n",
    "    train_x_gp = normalize(train_x, problem.bounds)\n",
    "\n",
    "    # form the output train_y data by concentenating ground truth train_obj with its feasibility value on the rightmost\n",
    "    # this is necessary since the surrogate GpyTorch model needs to model BOTH obj and con for predicted candidates\n",
    "    train_y = torch.cat([train_obj_noisy, train_con_noisy], dim=-1) # model takes noisy observations\n",
    "\n",
    "    # define and train surrogate models for objective and constraint\n",
    "    models = []\n",
    "    for i in range(train_y.shape[-1]):\n",
    "        models.append(SingleTaskGP(train_x_gp, train_y[..., i : i + 1], outcome_transform=Standardize(m=1)))\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "    \n",
    "    ##########    \n",
    "    \n",
    "    def create_idxr(i):\n",
    "        def idxr(Z):\n",
    "            return Z[..., i]\n",
    "\n",
    "        return idxr\n",
    "\n",
    "    def create_idxrs():\n",
    "        return [create_idxr(i=i) for i in range(problem.n_obj, problem.n_obj+problem.n_constr)]\n",
    "    \n",
    "    ########## ########## ########## start of iteration loop\n",
    "\n",
    "    for iteration in range(1, N_BATCH + 1):   \n",
    "\n",
    "        t3 = time.time()\n",
    "\n",
    "        ##########\n",
    "\n",
    "        fit_gpytorch_model(mll)  \n",
    "\n",
    "        # define the acqusition function for EIC if feas_weighting is false\n",
    "        acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "            model=model,\n",
    "            ref_point=-ref_point, # for computing HV, must flip for BoTorch\n",
    "            X_baseline=train_x_gp, # feed total list of train_x for this current iteration\n",
    "            sampler=SobolQMCNormalSampler(num_samples=128),  # determines how candidates are randomly proposed before selection\n",
    "            objective=IdentityMCMultiOutputObjective(outcomes=np.arange(problem.n_obj).tolist()), # optimize first n_obj col \n",
    "            constraints=create_idxrs(), # constraint on last n_constr col\n",
    "            prune_baseline=True, cache_pending=True)  # options for improving qNEHVI, keep these on\n",
    "\n",
    "        # propose best candidates given QMC and qNEHVI\n",
    "        qnehvi_x, _ = optimize_acqf(acq_function=acq_func,\n",
    "                                    bounds=standard_bounds, # since train_x was normalized\n",
    "                                    q=BATCH_SIZE, # no of candidates to propose in parallel, 12 is the max for a GTX1065\n",
    "                                    num_restarts=1, # no of restarts if q candidates fail to show improvement\n",
    "                                    raw_samples=256,  # pool of samples to choose the starting points from\n",
    "                                    options={\"batch_limit\": 5, \"maxiter\": 200}, # default arguments, not too sure about this yet\n",
    "                                 )\n",
    "\n",
    "        ##########\n",
    "        \n",
    "        max_pop = train_x_gp.shape[0]\n",
    "        \n",
    "        # redefine a new algo at each iteration\n",
    "        MOEAD_algo = MOEADIEpsilon(problem=jmetalproblem(),\n",
    "                          population_size=max_pop,\n",
    "                          crossover=DifferentialEvolutionCrossover(CR=1.0, F=0.5, K=0.5),\n",
    "                          mutation=PolynomialMutation(probability=1.0 / jmetalproblem().number_of_variables, distribution_index=20),\n",
    "                          aggregative_function=Tschebycheff(dimension=jmetalproblem().number_of_objectives),\n",
    "                          neighbor_size=int(max_pop/2),\n",
    "                          neighbourhood_selection_probability=0.9,\n",
    "                          max_number_of_replaced_solutions=2,\n",
    "                          weight_files_path='weights',\n",
    "                          #termination_criterion=StoppingByEvaluations(BATCH_SIZE),\n",
    "                         )\n",
    "        \n",
    "        \n",
    "        # initialize population\n",
    "        initial_solutions = []\n",
    "\n",
    "        for i in range(0, max_pop):\n",
    "            solution1 = FloatSolution(lower_bound=[0.0] * problem.n_var,\n",
    "                                      upper_bound=[1.0] * problem.n_var,\n",
    "                                      number_of_objectives=problem.n_obj,\n",
    "                                      number_of_constraints=problem.n_constr,\n",
    "                                     )\n",
    "            solution1.variables = train_x_gp[i].cpu().tolist()\n",
    "            solution1.objectives = train_obj_noisy[i].cpu().tolist()\n",
    "            solution1.constraints = train_con_noisy[i].cpu().tolist()\n",
    "            \n",
    "            initial_solutions.append(solution1)\n",
    "\n",
    "        MOEAD_algo.solutions = initial_solutions\n",
    "\n",
    "        # perform selection, crossover and mutation to form our offspring population\n",
    "        MOEAD_algo.permutation = Permutation(max_pop)\n",
    "        offspring_population = []\n",
    "\n",
    "        for q in range(256):\n",
    "            mating_population = MOEAD_algo.selection(MOEAD_algo.solutions)\n",
    "            offspring = MOEAD_algo.reproduction(mating_population)\n",
    "            offspring_population.append(offspring[0].variables)\n",
    "            \n",
    "        moead_x = torch.tensor(offspring_population, **tkwargs)\n",
    "        \n",
    "        ##########\n",
    "\n",
    "        candidates = torch.cat([qnehvi_x, moead_x])\n",
    "\n",
    "        acq_value_list = []\n",
    "\n",
    "        for i in range(0, candidates.shape[0]):\n",
    "            with torch.no_grad():\n",
    "                acq_value = acq_func(candidates[i].unsqueeze(dim=0))\n",
    "                acq_value_list.append(acq_value.item())\n",
    "\n",
    "        pred_hv_list = []\n",
    "        model.eval();\n",
    "\n",
    "        for i in range(0, candidates.shape[0]):\n",
    "            with torch.no_grad():\n",
    "                posterior = model.posterior(candidates[i].unsqueeze(0))\n",
    "                pred_y = posterior.mean\n",
    "                pred_hv = hv.compute(pred_y[:,:problem.n_obj])\n",
    "                pred_hv_list.append(pred_hv)\n",
    "\n",
    "        sorted_x = candidates.cpu().numpy()[np.lexsort((pred_hv_list, acq_value_list))]\n",
    "\n",
    "        ##########\n",
    "        \n",
    "        new_x = torch.tensor(sorted_x[-BATCH_SIZE:], **tkwargs) # take best BATCH_SIZE samples\n",
    "        # unormalize our training inputs back to original problem bounds\n",
    "        new_x =  unnormalize(new_x.detach(), bounds=problem.bounds)\n",
    "\n",
    "        # feed new proposed observations into objective func to get its new ground truth\n",
    "        new_obj, new_con = problem.evaluate(new_x)\n",
    "\n",
    "        # add noise, by default noise=0, so train_noisy = train, noise factor determines amt of std dev to add\n",
    "        new_obj_noisy = new_obj + noise*torch.randn_like(new_obj)\n",
    "        new_con_noisy = new_con + noise*torch.randn_like(new_con)\n",
    "\n",
    "        # update training points by concatenating the new values into their respective tensors\n",
    "        train_x = torch.cat([train_x, new_x])\n",
    "        train_obj = torch.cat([train_obj, new_obj])\n",
    "        train_con = torch.cat([train_con, new_con])\n",
    "        train_obj_noisy = torch.cat([train_obj_noisy, new_obj_noisy])\n",
    "        train_con_noisy = torch.cat([train_con_noisy, new_con_noisy])\n",
    "\n",
    "        # computing HV of current candidate list\n",
    "        is_feas = (train_con <= 0).all(dim=-1) # check whether points fit ALL (.all) constraint criteria\n",
    "        feas_train_obj = train_obj[is_feas] # take only points that fit the 1st check\n",
    "        if feas_train_obj.shape[0] > 0:\n",
    "            pareto_mask = is_non_dominated(feas_train_obj) # check for 2nd criteria: non-dominated, meaning new pareto optimal\n",
    "            pareto_y = feas_train_obj[pareto_mask] # take only points that fit the 2nd check\n",
    "            volume = hv.compute(pareto_y) # compute change in HV with new pareto optimal wrt to original ref point\n",
    "        else:\n",
    "            volume = 0.0\n",
    "\n",
    "        hvs.append(volume)\n",
    "\n",
    "        ##########\n",
    "\n",
    "        # update the surrogate models for next iteration\n",
    "        train_x_gp = normalize(train_x, problem.bounds) # dont forget to renormalize!\n",
    "        train_y = torch.cat([train_obj_noisy, train_con_noisy], dim=-1) # model takes noisy observations\n",
    "\n",
    "        models = []\n",
    "        for i in range(train_y.shape[-1]):\n",
    "            models.append(SingleTaskGP(train_x_gp, train_y[..., i : i + 1], outcome_transform=Standardize(m=1)))\n",
    "        model = ModelListGP(*models)\n",
    "        mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "        ##########\n",
    "\n",
    "        t4 = time.time()\n",
    "        if verbose:\n",
    "            print(\n",
    "                    f\"Batch {iteration:>2} of {N_BATCH}: Hypervolume = \"\n",
    "                    f\"{hvs[-1]:>4.2f}, \"\n",
    "                    f\"time = {t4-t3:>4.2f}s.\\n\"\n",
    "                    , end=\"\")\n",
    "\n",
    "        del new_x, new_obj, new_con, qnehvi_x, moead_x, new_obj_noisy, new_con_noisy, train_y\n",
    "        torch.cuda.empty_cache() # clear some memory here between each run/trial    \n",
    "        \n",
    "    t1 = time.time()\n",
    "    print(f\"Time taken in total: {t1-t0:>4.2f}s.\")   \n",
    "    \n",
    "    del models, model, mll\n",
    "    torch.cuda.empty_cache() # clear some memory here between each run/trial  \n",
    "    \n",
    "    # returns the HV score across iterations, total training set as an array\n",
    "    return hvs, torch.hstack([train_x, train_obj, train_con]).cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
