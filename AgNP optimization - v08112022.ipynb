{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fab6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic dependencies\n",
    "\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from numpy import savetxt\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "###########\n",
    "\n",
    "# torch dependencies\n",
    "import torch\n",
    "\n",
    "tkwargs = {\"dtype\": torch.double, # set as double to minimize zero error for cholesky decomposition error\n",
    "           #\"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")} # set tensors to GPU, if multiple GPUs please set cuda:x properly\n",
    "           \"device\": torch.device(\"cpu\")}\n",
    "\n",
    "torch.set_printoptions(precision=3)\n",
    "\n",
    "###########\n",
    "\n",
    "# botorch dependencies\n",
    "import botorch\n",
    "\n",
    "# data related\n",
    "from botorch.utils.sampling import draw_sobol_samples\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "\n",
    "# surrogate model specific\n",
    "from botorch.models.gp_regression import SingleTaskGP, FixedNoiseGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "\n",
    "# qNEHVI specific\n",
    "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n",
    "from botorch.acquisition.multi_objective.monte_carlo import qNoisyExpectedHypervolumeImprovement\n",
    "\n",
    "# utilities\n",
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "from botorch.utils.multi_objective.hypervolume import Hypervolume\n",
    "from typing import Optional\n",
    "from torch import Tensor\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=BadInitialCandidatesWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "###########\n",
    "\n",
    "# pymoo dependencies\n",
    "import pymoo\n",
    "\n",
    "from pymoo.factory import get_problem\n",
    "from pymoo.core.problem import ElementwiseProblem\n",
    "\n",
    "from pymoo.algorithms.moo.nsga3 import NSGA3\n",
    "from pymoo.algorithms.moo.unsga3 import UNSGA3\n",
    "from pymoo.factory import get_sampling, get_crossover, get_mutation, get_reference_directions, get_termination\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.core.termination import NoTermination\n",
    "\n",
    "from pymoo.core.problem import Problem\n",
    "\n",
    "from sympy.utilities.iterables import multiset_permutations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52185576",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## settings to change every day\n",
    "\n",
    "total_exp = 30 # total sets of experiments to run today\n",
    "\n",
    "algo = 'pure' # start from pure\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "########## settings to keep, used for some parts of the code\n",
    "\n",
    "exp_counter = 5 # current number of experiments done, start with 0 for initialization\n",
    "\n",
    "n_var = 5\n",
    "n_obj = 3\n",
    "n_constr = 2\n",
    "\n",
    "random_state = 1\n",
    "torch.manual_seed(random_state) # gives a consistent seed based on the trial number\n",
    "\n",
    "ref_point = torch.tensor([0, 0, 0], **tkwargs)\n",
    "hv=Hypervolume(ref_point=ref_point)\n",
    "\n",
    "initial_sample_size = 12 # no of initialized LHS samples\n",
    "nVirtualCond = 256  #number of additional virtual samples for training the constraint model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3448c41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully found! Proceeding with optimization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gpytorch\\lazy\\lazy_tensor.py:1811: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\BatchLinearAlgebra.cpp:2189.)\n",
      "  Linv = torch.triangular_solve(Eye, L, upper=False).solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment number 5 for pure BO , time taken: 11.04s.\n",
      "Total experiments run: 5\n",
      "Sleeping for 5 seconds before next experiment.\n",
      "File successfully found! Proceeding with optimization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1732\\3043205304.py:136: DeprecationWarning: Call to deprecated function (or staticmethod) get_reference_directions. (Please use `from pymoo.util.ref_dirs import get_reference_directions`)\n",
      "  ref_dirs=get_reference_directions(\"energy\", n_obj, BATCH_SIZE, seed=random_state),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "2\n",
      "[0, 1, 2, 3]\n",
      "[0, 1, 2, 3]\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "1\n",
      "[0, 1, 2, 3]\n",
      "[0, 1, 3, 2]\n",
      "[0, 1, 2, 3]\n",
      "[0, 1, 3, 2]\n",
      "Experiment number 5 for hybrid BO, time taken: 16.96s.\n",
      "Total experiments run: 5\n",
      "Sleeping for 5 seconds before next experiment.\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n",
      "No input file detected, sleeping for 30 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(input_file)\u001b[38;5;241m.\u001b[39mis_file() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo input file detected, sleeping for 30 sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m;\n\u001b[0;32m     13\u001b[0m initial_x_pandas \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while exp_counter < total_exp+1:\n",
    "    t0 = time.time()\n",
    "     \n",
    "    ##########\n",
    "    # load data from matlab/labview into jupyter       \n",
    "    if algo == 'pure':\n",
    "        input_file = \"Data Analysis/Algo1/Results_Algo1_Run\" + str(exp_counter) + \".csv\"\n",
    "        if Path(input_file).is_file() == False:\n",
    "            print(\"No input file detected, sleeping for 30 sec\")\n",
    "            time.sleep(30)\n",
    "            continue;\n",
    "            \n",
    "        initial_x_pandas = pd.read_csv(f\"{input_file}\", delimiter=',') \n",
    "        initial_x_torch = torch.tensor(initial_x_pandas.iloc[:,1:].values, **tkwargs) \n",
    "        \n",
    "    else:\n",
    "        input_file = \"Data Analysis/Algo2/Results_Algo2_Run\" + str(exp_counter) + \".csv\"\n",
    "        if Path(input_file).is_file() == False:\n",
    "            print(\"No input file detected, sleeping for 30 sec\")\n",
    "            time.sleep(30)\n",
    "            continue;\n",
    "            \n",
    "        initial_x_pandas = pd.read_csv(f\"{input_file}\", delimiter=',') \n",
    "        initial_x_torch = torch.tensor(initial_x_pandas.iloc[:,1:].values, **tkwargs) \n",
    "        \n",
    "    print(\"File successfully found! Proceeding with optimization.\")\n",
    "\n",
    "    ####################\n",
    "    # initial training data\n",
    "    train_x = initial_x_torch[:,:n_var]\n",
    "    train_obj = initial_x_torch[:,n_var:n_var+n_obj]\n",
    "    train_con = initial_x_torch[:,n_var+n_obj:n_var+n_obj+n_constr]\n",
    "\n",
    "    ####################\n",
    "    # normalization\n",
    "\n",
    "    # normalize inputs to [0,1] first before feeding into model\n",
    "    problem_bounds = torch.zeros(2, n_var, **tkwargs)\n",
    "    problem_bounds[0] = 0.6\n",
    "    problem_bounds[1] = 24.0\n",
    "\n",
    "    standard_bounds = torch.zeros(2, n_var, **tkwargs)\n",
    "    standard_bounds[1] = 1\n",
    "\n",
    "    ####################\n",
    "    # surrogate model\n",
    "    \n",
    "    # define and train surrogate models for objective and constraint\n",
    "    models = []\n",
    "    \n",
    "    # models for objective\n",
    "    train_x_gp = normalize(train_x, problem_bounds)\n",
    "    for i in range(train_obj.shape[-1]):\n",
    "        models.append(SingleTaskGP(train_x_gp, train_obj[..., i : i + 1], outcome_transform=Standardize(m=1)))\n",
    "    \n",
    "    # for constraints, including extra virtual data\n",
    "    input_file = \"virtual_x_gp_\" + str(nVirtualCond) + \".csv\"\n",
    "    virtual_x_pandas = pd.read_csv(f\"{input_file}\", delimiter=',') \n",
    "    virtual_x_gp = torch.tensor(virtual_x_pandas.iloc[:,:].values, **tkwargs)\n",
    "    \n",
    "    input_file = \"virtual_con_\" + str(nVirtualCond) + \".csv\"\n",
    "    virtual_con_pandas = pd.read_csv(f\"{input_file}\", delimiter=',') \n",
    "    virtual_con = torch.tensor(virtual_con_pandas.iloc[:,:].values, **tkwargs)\n",
    "    \n",
    "    virtual_x_gp1 = torch.vstack([train_x_gp, virtual_x_gp])\n",
    "    virtual_con1 = torch.vstack([train_con, virtual_con])\n",
    "    \n",
    "    for i in range(virtual_con1.shape[-1]):\n",
    "        models.append(SingleTaskGP(virtual_x_gp1, virtual_con1[..., i : i + 1], outcome_transform=Standardize(m=1)))\n",
    "\n",
    "    model = ModelListGP(*models)\n",
    "    mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    fit_gpytorch_model(mll) \n",
    "\n",
    "    ####################    \n",
    "    # acquisition function\n",
    "\n",
    "    def create_idxr(i):\n",
    "        def idxr(Z):\n",
    "            return Z[..., i]\n",
    "\n",
    "        return idxr\n",
    "\n",
    "    def create_idxrs():\n",
    "        return [create_idxr(i=i) for i in range(n_obj, n_obj+n_constr)]\n",
    "\n",
    "    if algo == 'pure':\n",
    "\n",
    "        acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "            model=model,\n",
    "            ref_point=ref_point, # for computing HV, must flip for BoTorch\n",
    "            X_baseline=train_x_gp, # feed total list of train_x for this current iteration\n",
    "            sampler=SobolQMCNormalSampler(num_samples=128),  # determines how candidates are randomly proposed before selection\n",
    "            objective=IdentityMCMultiOutputObjective(outcomes=np.arange(n_obj).tolist()), # optimize first n_obj col \n",
    "            constraints=create_idxrs(), # constraint on last n_constr col\n",
    "            prune_baseline=True, cache_pending=True)  # options for improving qNEHVI, keep these on\n",
    "\n",
    "        # propose candidates given defined qNEHVI acq func given model and latest observed training data\n",
    "        new_x, _ = optimize_acqf(\n",
    "                        acq_function=acq_func,\n",
    "                        bounds=standard_bounds, # since train_x was normalized\n",
    "                        q=BATCH_SIZE, # no of candidates to propose in parallel\n",
    "                        num_restarts=2, # no of restarts if q candidates fail to show improvement\n",
    "                        raw_samples=256,  # pool of samples to choose the starting points from\n",
    "                        options={\"batch_limit\": 5, \"maxiter\": 200}, # default arguments, not too sure about this yet\n",
    "                        )    \n",
    "\n",
    "    else:\n",
    "        # define acq_func for hybrid qNEHVI+U-NSGA-III\n",
    "        acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "            model=model,\n",
    "            ref_point=ref_point, # for computing HV, must flip for BoTorch\n",
    "            X_baseline=train_x_gp, # feed total list of train_x for this current iteration\n",
    "            sampler=SobolQMCNormalSampler(num_samples=128),  # determines how candidates are randomly proposed before selection\n",
    "            objective=IdentityMCMultiOutputObjective(outcomes=np.arange(n_obj).tolist()), # optimize first n_obj col \n",
    "            constraints=create_idxrs(), # constraint on last n_constr col\n",
    "            prune_baseline=True, cache_pending=True)  # options for improving qNEHVI, keep these on\n",
    "\n",
    "        # propose best candidates given QMC and qNEHVI\n",
    "        qnehvi_x, _ = optimize_acqf(acq_function=acq_func,\n",
    "                                    bounds=standard_bounds, # since train_x was normalized\n",
    "                                    q=4, # no of candidates to propose in parallel, 12 is the max for a GTX1065\n",
    "                                    num_restarts=1, # no of restarts if q candidates fail to show improvement\n",
    "                                    raw_samples=256,  # pool of samples to choose the starting points from\n",
    "                                    options={\"batch_limit\": 5, \"maxiter\": 200}, # default arguments, not too sure about this yet\n",
    "                                 )\n",
    "\n",
    "        # we pick out the best points so far to form parents\n",
    "        pareto_mask = is_non_dominated(train_obj)\n",
    "        pareto_y = -train_obj[pareto_mask]\n",
    "        pareto_x = train_x_gp[pareto_mask]\n",
    "        pareto_con = train_con[pareto_mask]\n",
    "\n",
    "        algorithm = UNSGA3(pop_size=256,\n",
    "                           ref_dirs=get_reference_directions(\"energy\", n_obj, BATCH_SIZE, seed=random_state),\n",
    "                           sampling=pareto_x.cpu().numpy(),\n",
    "                           #crossover=SimulatedBinaryCrossover(eta=30, prob=1.0),\n",
    "                           #mutation=PolynomialMutation(eta=20, prob=None),\n",
    "                          )\n",
    "\n",
    "        pymooproblem = Problem(n_var=n_var, n_obj=n_obj, n_constr=n_constr, \n",
    "                      xl=np.zeros(n_var), xu=np.ones(n_var))\n",
    "\n",
    "        algorithm.setup(pymooproblem, termination=NoTermination())\n",
    "        \n",
    "        # set the 1st population to the current evaluated population\n",
    "        pop = algorithm.ask()\n",
    "        pop.set(\"F\", pareto_y.cpu().numpy())\n",
    "        pop.set(\"G\", pareto_con.cpu().numpy())\n",
    "        algorithm.tell(infills=pop)\n",
    "\n",
    "        # propose children based on tournament selection -> crossover/mutation\n",
    "        newpop = algorithm.ask()\n",
    "        nsga3_x = torch.tensor(newpop.get(\"X\"), **tkwargs)\n",
    "\n",
    "        # total pool of candidates for sorting\n",
    "        candidates = torch.cat([qnehvi_x, nsga3_x])\n",
    "\n",
    "        acq_value_list = []\n",
    "        for i in range(0, candidates.shape[0]):\n",
    "            with torch.no_grad():\n",
    "                acq_value = acq_func(candidates[i].unsqueeze(dim=0))\n",
    "                acq_value_list.append(acq_value.item())\n",
    "\n",
    "        pred_hv_list = []\n",
    "        model.eval();\n",
    "        for i in range(0, candidates.shape[0]):\n",
    "            with torch.no_grad():\n",
    "                posterior = model.posterior(candidates[i].unsqueeze(0))\n",
    "                pred_y = posterior.mean\n",
    "                pred_hv = hv.compute(pred_y[:,:n_obj])\n",
    "                pred_hv_list.append(pred_hv)\n",
    "\n",
    "        # sorted\n",
    "        sorted_x = candidates.cpu().numpy()[np.lexsort((pred_hv_list, acq_value_list))]\n",
    "\n",
    "        # take best BATCH_SIZE samples from sorted pool\n",
    "        new_x = torch.tensor(sorted_x[-BATCH_SIZE:], **tkwargs)    \n",
    "\n",
    "    #################### \n",
    "    # normalization and repair\n",
    "\n",
    "    # unormalize our training inputs back to original problem bounds\n",
    "    new_x =  unnormalize(new_x.detach(), bounds=problem_bounds)\n",
    "    \n",
    "    # create a list to indicate if repair was done\n",
    "    repair_array = np.zeros((new_x.shape[0], 1))\n",
    "    \n",
    "    constraint_array1 = np.zeros((new_x.shape[0], 1))\n",
    "    constraint_array2 = np.zeros((new_x.shape[0], 1))\n",
    "    \n",
    "    # record prior constraint values\n",
    "    for i in range(new_x.shape[0]):\n",
    "        constraint_array1[i] = 0.3 - new_x[i,1]/new_x[i,4]\n",
    "        constraint_array2[i] = 2 - (new_x[i,1]/new_x[i,4]) - (new_x[i,3]/new_x[i,1])\n",
    "        \n",
    "    # repair inputs to feasibility\n",
    "    for i in range(new_x.shape[0]):\n",
    "        if 0.3 - new_x[i,1]/new_x[i,4] > 0:\n",
    "            new_x[i,4] = min(new_x[i,1]/0.3, new_x[i,4])\n",
    "            repair_array[i]+=0.5\n",
    "\n",
    "    for i in range(new_x.shape[0]):\n",
    "        if 2 - (new_x[i,1]/new_x[i,4]) - (new_x[i,3]/new_x[i,1]) > 0:\n",
    "            new_x[i,3] = max((2-new_x[i,1]/new_x[i,4])*new_x[i,1], new_x[i,3])\n",
    "            repair_array[i]+=1\n",
    "\n",
    "    #################### \n",
    "    t1 = time.time()\n",
    "    \n",
    "    # convert back to matlab/labview format for downloading\n",
    "    new_x_pandas = pd.DataFrame(new_x.cpu().numpy(), columns=['Qtsc', 'Qag', 'Qpva', 'Qseed', 'Qaa'])\n",
    "    new_x_pandas['Cond'] = new_x_pandas.index + BATCH_SIZE*exp_counter + initial_sample_size + 1\n",
    "    new_x_pandas['Repair'] = repair_array\n",
    "    new_x_pandas['Violation1'] = constraint_array1\n",
    "    new_x_pandas['Violation2'] = constraint_array2\n",
    "    new_x_pandas['Time'] =  t1-t0\n",
    "    new_x_pandas = new_x_pandas[['Cond', 'Qtsc', 'Qag', 'Qpva', 'Qseed', 'Qaa', 'Repair', 'Violation1', 'Violation2', 'Time']] \n",
    "        \n",
    "        \n",
    "    # Save new flowrate values in csv files before shuffling\n",
    "    if algo == 'pure':\n",
    "        output_file = \"Flowrates/Flowrates_Algo1_BS_Run\" + str(exp_counter+1)\n",
    "        new_x_pandas.to_csv(f'{output_file}.csv', index=False)\n",
    "        \n",
    "    else:\n",
    "        output_file = \"Flowrates/Flowrates_Algo2_BS_Run\" + str(exp_counter+1)\n",
    "        new_x_pandas.to_csv(f'{output_file}.csv', index=False)\n",
    "        \n",
    "         # Shuffle flowrate\n",
    "        input_file = \"Flowrates/Flowrates_Algo1_BS_Run\" + str(exp_counter+1) + \".csv\"            \n",
    "        dataAlgo1_pandas = pd.read_csv(f\"{input_file}\", delimiter=',') \n",
    "        dataAlgo1_torch = torch.tensor(dataAlgo1_pandas.iloc[:,:].values, **tkwargs)\n",
    "\n",
    "        input_file = \"Flowrates/Flowrates_Algo2_BS_Run\" + str(exp_counter+1) + \".csv\"            \n",
    "        dataAlgo2_pandas = pd.read_csv(f\"{input_file}\", delimiter=',') \n",
    "        dataAlgo2_torch = torch.tensor(dataAlgo2_pandas.iloc[:,:].values, **tkwargs)\n",
    "   \n",
    "        iOrder = [0,4,1,5,2,6,3,7]\n",
    "        iSave = 4\n",
    "        pSave = [0,1,2,3]\n",
    "        qSave = [0,1,2,3]\n",
    "            \n",
    "        for p in multiset_permutations([0,1,2,3]):\n",
    "            for q in multiset_permutations([0,1,2,3]):\n",
    "                dataAlgo_torch = torch.vstack([dataAlgo1_torch[p,1:6], dataAlgo2_torch[q,1:6]])\n",
    "                dataAlgoOrder_torch = dataAlgo_torch[iOrder]\n",
    "\n",
    "                Test = dataAlgoOrder_torch\n",
    "                Test[Test <= 4] = 1\n",
    "                Test[Test > 4] = 0\n",
    "\n",
    "                # Cumulative sum\n",
    "                cumsumsave = torch.zeros(2*BATCH_SIZE-1, 1, **tkwargs)\n",
    "                for i in range(2*BATCH_SIZE-1):\n",
    "                    cumsum = torch.sum(Test[i:i+2],0)\n",
    "                    cumsumsave[i] = max(cumsum)\n",
    "\n",
    "                if(max(cumsumsave) < 2) & (iSave > 1):\n",
    "                    pSave = p\n",
    "                    qSave = q\n",
    "                    iSave = 1\n",
    "                    print(Test)\n",
    "                    print(iSave)\n",
    "                    print(pSave)\n",
    "                    print(qSave)\n",
    "                else:\n",
    "                    # Cumulative sum\n",
    "                    cumsumsave = torch.zeros(2*BATCH_SIZE-2, 1, **tkwargs)\n",
    "                    for i in range(2*BATCH_SIZE-2):\n",
    "                        cumsum = torch.sum(Test[i:i+3],0)\n",
    "                        cumsumsave[i] = max(cumsum)\n",
    "                    \n",
    "                    if(max(cumsumsave) < 3) & (iSave > 2):\n",
    "                        pSave = p\n",
    "                        qSave = q\n",
    "                        iSave = 2\n",
    "                        print(Test)\n",
    "                        print(iSave)\n",
    "                        print(pSave)\n",
    "                        print(qSave)\n",
    "                    else:\n",
    "                        # Cumulative sum\n",
    "                        cumsumsave = torch.zeros(2*BATCH_SIZE-3, 1, **tkwargs)\n",
    "                        for i in range(2*BATCH_SIZE-3):\n",
    "                            cumsum = torch.sum(Test[i:i+4],0)\n",
    "                            cumsumsave[i] = max(cumsum)\n",
    "\n",
    "                        if(max(cumsumsave) < 4) & (iSave > 3):\n",
    "                            pSave = p\n",
    "                            qSave = q\n",
    "                            iSave = 3\n",
    "                            print(Test)\n",
    "                            print(iSave)\n",
    "                            print(pSave)\n",
    "                            print(qSave)\n",
    "                \n",
    "        print(pSave)\n",
    "        print(qSave)\n",
    "        \n",
    "        dataAlgo_torch = torch.vstack([dataAlgo1_torch[pSave], dataAlgo2_torch[qSave]])\n",
    "        dataAlgoOrder_torch = dataAlgo_torch[iOrder]\n",
    "        new_dataAlgo1_pandas = pd.DataFrame(dataAlgo1_torch[pSave,:].cpu().numpy(), columns=['Cond', 'Qtsc', 'Qag', 'Qpva', 'Qseed', 'Qaa', 'Repair', 'Violation1', 'Violation2', 'Time'])\n",
    "        new_dataAlgo2_pandas = pd.DataFrame(dataAlgo2_torch[qSave,:].cpu().numpy(), columns=['Cond', 'Qtsc', 'Qag', 'Qpva', 'Qseed', 'Qaa', 'Repair', 'Violation1', 'Violation2', 'Time'])\n",
    "        \n",
    "        new_dataAlgo1_pandas['Cond'] = new_dataAlgo1_pandas.index + BATCH_SIZE*exp_counter + initial_sample_size + 1\n",
    "        new_dataAlgo2_pandas['Cond'] = new_dataAlgo2_pandas.index + BATCH_SIZE*exp_counter + initial_sample_size + 1\n",
    "\n",
    "\n",
    "        # Save new flowrate values in csv files after shuffling\n",
    "        output_file = \"Flowrates/Flowrates_Algo1_Run\" + str(exp_counter+1)\n",
    "        new_dataAlgo1_pandas.to_csv(f'{output_file}.csv', index=False)\n",
    "\n",
    "        output_file = \"Flowrates/Flowrates_Algo2_Run\" + str(exp_counter+1)\n",
    "        new_dataAlgo2_pandas.to_csv(f'{output_file}.csv', index=False)\n",
    "\n",
    "        \n",
    "    #################### \n",
    "    # end of loop\n",
    "        \n",
    "    if algo == 'pure':\n",
    "        print(f\"Experiment number {exp_counter} for pure BO , time taken: {t1-t0:>4.2f}s.\\nTotal experiments run: {exp_counter}\")\n",
    "        algo = 'hybrid' # switch to next algo\n",
    "\n",
    "    else:\n",
    "        print(f\"Experiment number {exp_counter} for hybrid BO, time taken: {t1-t0:>4.2f}s.\\nTotal experiments run: {exp_counter}\")\n",
    "        algo = 'pure'  # switch to next algo\n",
    "        exp_counter+=1\n",
    "    \n",
    "    print(\"Sleeping for 5 seconds before next experiment.\")\n",
    "    time.sleep(5) # pause for x sec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e0aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
